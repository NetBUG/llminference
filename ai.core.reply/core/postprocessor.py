#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2025/02/17 15:00
# @Author  : Oleg Urzhumtsev aka @netbug
# @Site    : https://github.com/NetBUG/llminference
# @File    : core/postprocessor.py
# This file contains filtering postprocessor for AI Core Reply

import random
import re
from typing import List, Tuple, Dict

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

from core.utils.blacklists import load_lists
from instance.logger import logger as base_logger
from instance.parameters import FilteringAction, FilteringParameters

class Postprocessor():
    error_count = 0
    model_toxic = None
    tokenizer_toxic = None
    _ready = False

    def __init__(self, device: str = "cuda:0"):
        self.device = device
        self.tokenizer_toxic = None
        self.model_toxic = None
        self._ready = False
        self.logger = base_logger.bind(corr_id='POSTPROC')

        stub_file = FilteringParameters.stub_templates_file
        self.localObject = load_lists(stub_file)
        self.stubs = self.localObject["postfilter_blacklist_reasons"] \
            if "postfilter_blacklist_reasons" in self.localObject else []

        self.load_toxicity_model()

        self.filtering_action = FilteringParameters.preprocessor_action

        if self.filtering_action == FilteringAction.FILTER:
            raise NotImplementedError("Filtering action FILTER is not implemented!")
        if self.filtering_action == FilteringAction.STUB and len(self.stubs) == 0:
            self.logger.warning(f"Filtering action is set to STUB, but the field `prefilter_blacklist_reasons` is empty in {self.filter_blacklist_file}!")

    def is_ready(self) -> bool:
        return self._ready

    def load_toxicity_model(self):
        self.toxicity_model_path = FilteringParameters.model_name
        self.logger.debug("Loading sequence classification model from %s to %s" % (self.toxicity_model_path, self.device))

        self._ready = False
        del self.tokenizer_toxic
        del self.model_toxic
        self.tokenizer_toxic = AutoTokenizer.from_pretrained(self.toxicity_model_path)
        self.model_toxic = AutoModelForSequenceClassification.from_pretrained(self.toxicity_model_path).to(self.device)

        if self.model_toxic:
            self._ready = True
        else:
            raise Exception("Error loading the toxicity model!")


    def output_post_processing(self, text: str) -> str:
        """
        Replaces and removes special characters, splits the text into sentences for one string
        """
        text = re.sub(f"<s>", "", text)
        text = text.split("</s>")[0]

        sentences = text.strip().split("\n")    # For simplicity

        out = " ".join(sentences[0:-1])

        out = out.strip("\n").strip()
        out = out.strip(".").strip()
        out = out.replace("  ", " ")
        return out


    def get_toxic_score(self, history: List[str]) -> float:
        if not self.model_toxic:
            self.logger.error("Model for toxicity filtering not loaded!")
        if not self.model_toxic.config:
            self.logger.error("Model for toxicity filtering does not have a config!")

        self.logger.trace(f"history: {history}")
        text = "\n".join(history)

        tokens = self.tokenizer_toxic.encode(text, return_tensors='pt').to(self.device)

        with torch.no_grad():
            out = self.model_toxic(tokens).logits.squeeze()

        toxic_score = out.max().item()

        return toxic_score


    def assess_response(self, history: List[str], response: str) -> bool:
        """ 
        Assess the response according to the preprocessor action
        @param history: input history (single or multiple query chunks)
        @param response: response candidate generated by the model
        @return: True if message is filtered by the model
        """
        toxic_scores = [self.get_toxic_score(history + [response]), \
                       self.get_toxic_score([response])]

        self.logger.debug(f"toxic_scores: {toxic_scores}")
        return min(toxic_scores) > FilteringParameters.postprocessor_filtering_threshold

    def filter_context(self, history: List[str], responses: List[str]) -> Tuple[str, Dict]:
        """
        Filters the text according to the preprocessor action
        @param history: input history (single or multiple query chunks)
        @param responses: list of response candidates generated by the model
        @return: filtered text; True if message contains blacklisted words
        """
        if self.filtering_action == FilteringAction.NONE:
            return random.choice(responses), False

        responses = [self.output_post_processing(response) for response in responses]
        responses = [response for response in responses if len(response) > 0]
        self.logger.debug(f"Generated responses: {responses}")

        filtered_responses = [resp for resp in responses if self.assess_response(history, resp)]
        self.logger.debug(f"Remaining responses: {filtered_responses}")

        if len(filtered_responses) > 0:
            return random.choice(filtered_responses), False
        else:
            return random.choice(self.stubs), True
